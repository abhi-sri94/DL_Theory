{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1\n",
        "\n",
        "How does unsqueeze help us to solve certain broadcasting problems?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 1 -\n",
        "\n",
        "In PyTorch, the unsqueeze method is used to add a new dimension to a tensor, effectively increasing its rank. This can be particularly helpful in solving broadcasting problems when you need to align tensor dimensions for elementwise operations.\n",
        "\n",
        "Here's how unsqueeze helps in solving certain broadcasting problems:\n",
        "\n",
        "1) `Aligning Dimensions` :\n",
        "\n",
        "- When working with tensors of different ranks, you may encounter situations where one tensor has fewer dimensions than the other.\n",
        "\n",
        "- `unsqueeze` allows you to add new dimensions to the tensor with fewer dimensions, aligning its shape with the higher-dimensional tensor.\n",
        "\n",
        "2) `Broadcasting with Scalars` :\n",
        "\n",
        "- Scalars in PyTorch are considered tensors with rank 0.\n",
        "\n",
        "- When you want to perform elementwise operations involving a scalar and another tensor, you can use `unsqueeze` to add a new dimension to the scalar tensor, making it compatible with the shape of the other tensor.\n",
        "\n",
        "3) `Preventing Squeezing Issues` :\n",
        "\n",
        "- Sometimes, after certain operations, you might end up with a tensor with a size-1 dimension that you want to retain.\n",
        "\n",
        "- `unsqueeze` helps in preventing dimensions from being squeezed out during subsequent operations.\n",
        "\n",
        "Here's a simple example:"
      ],
      "metadata": {
        "id": "3ARjvaJN5A4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example tensors\n",
        "tensor_A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "scalar_B = torch.tensor(10)\n",
        "\n",
        "# Broadcasting without unsqueeze (may result in an error)\n",
        "result_without_unsqueeze = tensor_A + scalar_B\n",
        "\n",
        "# Broadcasting with unsqueeze\n",
        "scalar_B_expanded = scalar_B.unsqueeze(0)  # Add a new dimension\n",
        "result_with_unsqueeze = tensor_A + scalar_B_expanded\n",
        "\n",
        "# Display the results\n",
        "print(\"Result without unsqueeze (may result in an error):\\n\", result_without_unsqueeze)\n",
        "print(\"\\nResult with unsqueeze:\\n\", result_with_unsqueeze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsjID5WK3-TY",
        "outputId": "65803786-c2ab-4787-8802-cf9ef1ac3b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result without unsqueeze (may result in an error):\n",
            " tensor([[11, 12, 13],\n",
            "        [14, 15, 16]])\n",
            "\n",
            "Result with unsqueeze:\n",
            " tensor([[11, 12, 13],\n",
            "        [14, 15, 16]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2\n",
        "\n",
        "How can we use indexing to do the same operation as unsqueeze?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 2 -\n",
        "\n",
        "You can use indexing to achieve the same result as `unsqueeze` by manipulating the dimensions of the tensor. In PyTorch, indexing can be employed to add new dimensions or expand existing ones.\n",
        "\n",
        "Here's an example:"
      ],
      "metadata": {
        "id": "Yhx06mNt4Oxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example tensors\n",
        "tensor_A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "scalar_B = torch.tensor(10)\n",
        "\n",
        "# Broadcasting with unsqueeze() to add a new dimension\n",
        "scalar_B_expanded = scalar_B.unsqueeze(0)  # Add a new dimension at the beginning\n",
        "result_with_unsqueeze = tensor_A + scalar_B_expanded\n",
        "\n",
        "# Display the results\n",
        "print(\"Result with unsqueeze() to add a new dimension:\\n\", result_with_unsqueeze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NmaC6b34-ap",
        "outputId": "261448d3-7462-4d90-99a7-0c24f24878eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result with unsqueeze() to add a new dimension:\n",
            " tensor([[11, 12, 13],\n",
            "        [14, 15, 16]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3\n",
        "\n",
        "How do we show the actual contents of the memory used for a tensor?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 3 -\n",
        "\n",
        "To inspect the actual contents of the memory used for a PyTorch tensor, you can use the `.numpy()` method to convert the tensor to a NumPy array and then print the array. Additionally, you can use the `.tolist()` method to convert the tensor to a Python list.\n",
        "\n",
        "Here's an example:"
      ],
      "metadata": {
        "id": "dhF5Two85Q2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a tensor\n",
        "tensor_A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Convert the tensor to NumPy array and print\n",
        "numpy_array = tensor_A.numpy()\n",
        "print(\"NumPy Array:\\n\", numpy_array)\n",
        "\n",
        "# Convert the tensor to Python list and print\n",
        "python_list = tensor_A.tolist()\n",
        "print(\"\\nPython List:\\n\", python_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meiSrg894_GL",
        "outputId": "a5ff5873-ce89-4ad4-ea65-e9762016cc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Array:\n",
            " [[1 2 3]\n",
            " [4 5 6]]\n",
            "\n",
            "Python List:\n",
            " [[1, 2, 3], [4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tensor:\\n\", tensor_A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSEkRcq65bmB",
        "outputId": "4f8cb03d-47e8-4b5a-ffeb-5c9556199686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4\n",
        "\n",
        "When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
        "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
        "code in a notebook.)\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 4 -\n",
        "\n",
        "When adding a vector of size 3 to a matrix of size 3x3 in PyTorch, the broadcasting rules are applied. The elements of the vector will be added to each row of the matrix. Each element in the vector is broadcasted across the corresponding row of the matrix.\n",
        "\n",
        "Here's a code snippet to illustrate this:"
      ],
      "metadata": {
        "id": "5V5OZBnV5vrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a matrix of size 3x3\n",
        "matrix_A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Create a vector of size 3\n",
        "vector_B = torch.tensor([10, 20, 30])\n",
        "\n",
        "# Adding the vector to the matrix\n",
        "result = matrix_A + vector_B\n",
        "\n",
        "# Display the results\n",
        "print(\"Matrix A:\\n\", matrix_A)\n",
        "print(\"\\nVector B:\\n\", vector_B)\n",
        "print(\"\\nResult (each row of the matrix added with the vector):\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGf1ne3_5ffn",
        "outputId": "b592a1d7-10b9-4f74-c18c-0516ac291c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n",
            "\n",
            "Vector B:\n",
            " tensor([10, 20, 30])\n",
            "\n",
            "Result (each row of the matrix added with the vector):\n",
            " tensor([[11, 22, 33],\n",
            "        [14, 25, 36],\n",
            "        [17, 28, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5\n",
        "\n",
        "Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 5 -\n",
        "\n",
        "Broadcasting and expand_as in PyTorch are designed to be memory-efficient operations. They do not result in the creation of new copies of the data but rather provide a way to virtually expand or broadcast the existing data to perform element-wise operations.\n",
        "\n",
        "Here's why these operations are memory-efficient:\n",
        "\n",
        "1) **No Copying of Data** : Broadcasting and `expand_as` operations do not create new copies of the data. They operate on the existing tensors without duplicating the underlying memory.\n",
        "\n",
        "2) **Shared Memory** : The expanded or broadcasted tensor shares the memory with the original tensor. No additional memory is allocated for the expanded tensor; instead, it points to the same memory as the original tensor.\n",
        "\n",
        "3) **Lazy Evaluation** :\n",
        "\n",
        "- PyTorch, like many other modern deep learning frameworks, uses lazy evaluation. This means that operations are not immediately executed but are recorded in a computation graph. The actual computation is performed when the result is needed.\n",
        "\n",
        "- Broadcasting and expand_as are part of this lazy evaluation strategy. They are efficient because they don't perform the actual computation until necessary.\n",
        "\n",
        "4) **Efficient Implementation** : PyTorch is designed to optimize memory usage and computation efficiency. The underlying implementation of broadcasting and related operations is carefully optimized to minimize unnecessary memory allocations.\n",
        "\n",
        "While these operations are memory-efficient, it's important to note that the efficiency comes from avoiding unnecessary memory copies. However, if you explicitly create a new tensor by using functions like `clone()` , `detach()` , or `contiguous()` , it may lead to increased memory use."
      ],
      "metadata": {
        "id": "DQ3l0tNZ6AJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6\n",
        "\n",
        "Implement matmul using Einstein summation.\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 6 -\n",
        "\n",
        "Here's an implementation of matrix multiplication (matmul) using Einstein summation in PyTorch:"
      ],
      "metadata": {
        "id": "eby3pWwz6vd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def matmul_einsum(A, B):\n",
        "    # Ensure the number of columns in A matches the number of rows in B\n",
        "    assert A.shape[1] == B.shape[0], \"Incompatible shapes for matrix multiplication\"\n",
        "\n",
        "    # Einstein summation for matrix multiplication\n",
        "    # The notation 'ij,jk->ik' implies summation over the repeated index (j)\n",
        "    result = torch.einsum('ij,jk->ik', A, B)\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example matrices\n",
        "matrix_A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "matrix_B = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
        "\n",
        "# Perform matrix multiplication using Einstein summation\n",
        "result = matmul_einsum(matrix_A, matrix_B)\n",
        "\n",
        "# Display the results\n",
        "print(\"Matrix A:\\n\", matrix_A)\n",
        "print(\"\\nMatrix B:\\n\", matrix_B)\n",
        "print(\"\\nResult of matmul using Einstein summation:\\n\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mszex9-555uI",
        "outputId": "f605aded-4d87-43e6-c2c0-52473e2c74e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Matrix B:\n",
            " tensor([[ 7,  8],\n",
            "        [ 9, 10],\n",
            "        [11, 12]])\n",
            "\n",
            "Result of matmul using Einstein summation:\n",
            " tensor([[ 58,  64],\n",
            "        [139, 154]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quetsion 7\n",
        "\n",
        "What does a repeated index letter represent on the lefthand side of einsum?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 7 -\n",
        "\n",
        "In the Einstein summation notation used in torch.einsum, a repeated index letter on the left-hand side represents a summation or contraction over that index. The repeated index implies that the corresponding dimensions are summed over.\n",
        "\n",
        "Let's break down the notation `'ij,jk->ik'` as an example:\n",
        "\n",
        "- `i` is a free index, meaning it is `not repeated` on the left-hand side, and it appears in the output.\n",
        "\n",
        "- `j` is a `repeated` index, and it appears on both sides of the arrow `(->)` . This implies a summation over the dimensions associated with `j` .\n",
        "\n",
        "- `k` is a free index, appearing on the `left-hand side` and in the output.\n",
        "\n",
        "The notation indicates the following operations:\n",
        "\n",
        "1) Summation over the repeated index `j` .\n",
        "\n",
        "2) The result is a tensor with indices `i` and `k` .\n",
        "\n",
        "In terms of matrix multiplication, the notation `'ij,jk->ik'` corresponds to the multiplication of two matrices, where the shared index `j` is summed over.\n",
        "\n",
        "This is consistent with the Einstein summation convention for expressing tensor operations concisely.\n",
        "\n",
        "Here's how the Einstein summation notation corresponds to matrix multiplication:\n",
        "\n",
        "- `ij` : Indices for the elements of the `first` matrix.\n",
        "\n",
        "- `jk` : Indices for the elements of the `second` matrix.\n",
        "\n",
        "- `ik` : Indices for the elements of the `result` matrix.\n",
        "\n",
        "The repeated index `j` signifies the summation over the corresponding dimension, which is the inner dimension of the matrices in the context of matrix multiplication."
      ],
      "metadata": {
        "id": "xuDH-DoV7NB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8\n",
        "\n",
        "What are the three rules of Einstein summation notation? Why?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 8 -\n",
        "\n",
        "The Einstein summation notation follows three fundamental rules:\n",
        "\n",
        "1) **Repeating Indices Implies Summation** : If an index appears twice (repeated) in a term, it implies summation over that index. The repeated index is summed over all possible values.\n",
        "\n",
        "2) **Free Indices Are Not Summed** : If an index appears only once in a term (not repeated), it is considered a free index. Free indices are not summed; they are included in the output as they are.\n",
        "\n",
        "3) **Matching Indices on Either Side of the Arrow** : In the notation `...a...->...b...` , any index a on the left side of the arrow must match the index b on the right side. This ensures that the dimensions align correctly for the operation."
      ],
      "metadata": {
        "id": "pPIJcUuK-XlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9\n",
        "\n",
        "What are the forward pass and backward pass of a neural network?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 9 -\n",
        "\n",
        "The forward pass and backward pass are two essential steps in the training of a neural network. They are also known as the feedforward step and backpropagation step, respectively.\n",
        "\n",
        "1) **Forward pass** : In the forward pass, the input data is passed through the neural network to produce an output. Each neuron in the network receives input from the previous layer, applies an activation function to it, and outputs the result to the next layer. This process is repeated for each layer until the output layer produces the final result. During the forward pass, the weights and biases of the network are fixed and not updated.\n",
        "\n",
        "2) **Backward pass** : In the backward pass, the error in the output is calculated and propagated back through the network to adjust the weights and biases. This process is called backpropagation. The goal of backpropagation is to update the weights and biases in a way that reduces the error in the output. This is done by computing the gradient of the error with respect to each weight and bias in the network. The gradient is then used to update the weights and biases using an optimization algorithm such as gradient descent.\n",
        "\n",
        "The forward pass and backward pass are repeated multiple times during the training process, with each iteration updating the weights and biases of the network to improve its performance. The goal is to minimize the error between the network's predicted output and the true output.\n",
        "\n",
        "Overall, the forward pass and backward pass are essential components of training a neural network. The forward pass computes the output of the network given the input, and the backward pass updates the network's weights and biases to improve its performance."
      ],
      "metadata": {
        "id": "jqe8c3Ox-xMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10\n",
        "\n",
        "Why do we need to store some of the activations calculated for intermediate layers in the\n",
        "forward pass?\n",
        "\n",
        ".................\n",
        "\n",
        "Answer 10 -\n",
        "\n",
        "Storing activations calculated for intermediate layers during the forward pass is crucial for efficient and accurate training of neural networks. This process is often referred to as \"activation caching\" or \"activations memoization.\" There are several reasons why it is necessary:\n",
        "\n",
        "1) **Backpropagation and Gradients** : During the backward pass (backpropagation), the gradients of the loss with respect to the model parameters are computed by propagating the error backward through the network. The gradients depend on the activations calculated during the forward pass. Storing these intermediate activations allows us to efficiently compute gradients without recomputing the entire forward pass.\n",
        "\n",
        "2) **Memory Efficiency** : Activations can consume a significant amount of memory, especially in deep neural networks with numerous layers. By caching intermediate activations, we avoid the need to recalculate them during the backward pass. This improves memory efficiency and reduces the overall computational cost.\n",
        "\n",
        "3) **Multiple Computations of Same Activations** : In some architectures or optimization algorithms, the same intermediate activations may be needed multiple times during the backward pass. Caching these values avoids redundant computations, saving computational resources.\n",
        "\n",
        "4) **Efficient Implementation of Skip Connections** : Skip connections or residual connections, commonly used in architectures like ResNet, involve adding the input of a layer to its output. Storing intermediate activations is essential for efficiently implementing skip connections during the backward pass.\n",
        "\n",
        "5) **Facilitates Debugging and Analysis** : Storing intermediate activations allows for easy inspection and debugging of the network's behavior. It helps researchers and practitioners analyze the features learned by different layers and diagnose issues in the model."
      ],
      "metadata": {
        "id": "iwOLnk_S-5Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 11\n",
        "What is the downside of having activations with a standard deviation too far away from 1?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 11 -\n",
        "\n",
        "Having activations with a standard deviation too far away from 1 in a neural network can lead to several issues during training. The standard deviation of activations is a measure of the spread or variability of the values in a layer. Here are the main downsides:\n",
        "\n",
        "1) **Vanishing and Exploding Gradients** : If the standard deviation is too small (activations are too close to zero), it may lead to vanishing gradients during backpropagation. This is problematic because small gradients result in minimal updates to the model parameters, hindering learning.\n",
        "Conversely, if the standard deviation is too large (activations are too far from zero), it may lead to exploding gradients. This can cause large updates to the model parameters, leading to instability and difficulty in convergence.\n",
        "\n",
        "2) **Saturated Activation Functions** : In the case of activation functions like sigmoid or tanh, when inputs are too far from zero, the functions saturate, meaning they squash inputs to very small or very large values. Saturated activation functions can lead to the vanishing gradient problem and hinder learning.\n",
        "\n",
        "3) **Difficulty in Learning Representations** : Activations that are too small or too large may result in the network struggling to learn meaningful representations of the input data. Learning becomes challenging when the activations do not convey useful information to subsequent layers.\n",
        "\n",
        "4) **Gradient Descent Instability** : Large variations in activation values can make the optimization landscape more challenging for gradient descent. The optimization process may oscillate or diverge, making it difficult to find an optimal solution.\n",
        "\n",
        "5) **Normalization Challenges** : Techniques like batch normalization rely on the assumption that activations have a certain level of variability. Extreme values can interfere with the normalization process and reduce the effectiveness of normalization techniques."
      ],
      "metadata": {
        "id": "nv23155N_k9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 12\n",
        "\n",
        "How can weight initialization help avoid this problem?\n",
        "\n",
        ".................\n",
        "\n",
        "Answer 12 -\n",
        "\n",
        "Weight initialization is a crucial aspect of training neural networks, and it can help avoid issues related to vanishing or exploding gradients. Proper weight initialization ensures that the activations during the forward pass and the gradients during backpropagation are within a reasonable range, making the optimization process more stable. Here are some commonly used weight initialization techniques and how they address the problem:\n",
        "\n",
        "1) **Zero Initialization** :\n",
        "\n",
        "- Initializing all weights to zero is generally not recommended because it leads to symmetry issues. If all weights are the same, neurons in the network will learn the same features, and the model won't be able to capture complex patterns.\n",
        "\n",
        "2) **Random Initialization (Glorot / Xavier Initialization)** :\n",
        "\n",
        "- Glorot or Xavier initialization is a popular method that sets the initial weights to random values drawn from a distribution with zero mean and a variance calculated based on the number of input and output units in the layer. This initialization helps balance the variances of activations during the forward and backward passes.\n",
        "\n",
        "- Xavier initialization is particularly effective for activation functions like tanh or sigmoid.\n",
        "\n",
        "3) **He Initialization** :\n",
        "\n",
        "- He initialization is similar to Xavier initialization but is designed for activation functions with rectified linear units (ReLUs). It sets the initial weights to random values drawn from a distribution with zero mean and a variance calculated based on the number of input units in the layer.\n",
        "\n",
        "- He initialization helps prevent the vanishing gradient problem associated with ReLUs.\n",
        "\n",
        "4) **LeCun Initialization** :\n",
        "\n",
        "- LeCun initialization is specifically designed for hyperbolic tangent (tanh) activation functions. It sets the initial weights to random values drawn from a distribution with zero mean and a variance calculated based on the number of input units in the layer.\n",
        "\n",
        "By using appropriate weight initialization techniques, the neural network can start training with activations that are neither too small nor too large. This helps in mitigating issues such as vanishing or exploding gradients, leading to more stable and efficient training. The choice of initialization method often depends on the activation functions used in the network."
      ],
      "metadata": {
        "id": "I5e6tEttoNtf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvuLeFnb6_Mr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}