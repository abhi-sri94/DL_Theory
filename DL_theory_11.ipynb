{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1\n",
        "\n",
        "Write the Python code to implement a single neuron.\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 1 -\n",
        "\n"
      ],
      "metadata": {
        "id": "jLKeBiJWo2IL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TBt-fRdwoZge"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SingleNeuron:\n",
        "    def __init__(self, input_size):\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.random.rand(input_size)\n",
        "        self.bias = np.random.rand()\n",
        "\n",
        "    def activate(self, inputs):\n",
        "        # Perform weighted sum and apply activation function (linear in this case)\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "        output = self.activation_function(weighted_sum)\n",
        "        return output\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        # Linear activation function\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a single neuron with 3 input weights\n",
        "    neuron = SingleNeuron(input_size=3)\n",
        "\n",
        "    # Provide input values\n",
        "    inputs = np.array([0.5, 0.3, 0.8])\n",
        "\n",
        "    # Activate the neuron\n",
        "    output = neuron.activate(inputs)\n",
        "\n",
        "    # Display the result\n",
        "    print(\"Input:\", inputs)\n",
        "    print(\"Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qBg-Ct2pQsx",
        "outputId": "58849ecb-007c-488e-e011-7d5428140997"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [0.5 0.3 0.8]\n",
            "Output: 1.2371180238169326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2\n",
        "\n",
        "Write the Python code to implement ReLU.\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 2 -"
      ],
      "metadata": {
        "id": "IzG4iK60peD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    # ReLU activation function\n",
        "    return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "Pgd58jkIpYfK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example input values\n",
        "    input_values = np.array([-2, -1, 0, 1, 2])\n",
        "\n",
        "    # Apply ReLU activation\n",
        "    output_values = relu(input_values)\n",
        "\n",
        "    # Display the result\n",
        "    print(\"Input values:\", input_values)\n",
        "    print(\"Output values after ReLU activation:\", output_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLb1jsFopwbz",
        "outputId": "c251e262-9ea6-45e5-fc9a-43eb9a0df3d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input values: [-2 -1  0  1  2]\n",
            "Output values after ReLU activation: [0 0 0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3\n",
        "\n",
        "Write the Python code for a dense layer in terms of matrix multiplication.\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 3 -"
      ],
      "metadata": {
        "id": "2uVHvYAup7CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.rand(input_size, output_size)\n",
        "        self.biases = np.random.rand(output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform matrix multiplication and add biases\n",
        "        weighted_sum = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "        # Apply activation function (ReLU in this case)\n",
        "        output = self.relu_activation(weighted_sum)\n",
        "        return output\n",
        "\n",
        "    def relu_activation(self, x):\n",
        "        # ReLU activation function\n",
        "        return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "CJzr1DcApyud"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a dense layer with 3 input neurons and 2 output neurons\n",
        "    dense_layer = DenseLayer(input_size=3, output_size=2)\n",
        "\n",
        "    # Provide input values\n",
        "    inputs = np.array([0.5, 0.3, 0.8])\n",
        "\n",
        "    # Forward pass through the dense layer\n",
        "    output = dense_layer.forward(inputs)\n",
        "\n",
        "    # Display the result\n",
        "    print(\"Input values:\", inputs)\n",
        "    print(\"Output values after dense layer with ReLU activation:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3-i4y33qG5u",
        "outputId": "15962e87-e71e-400c-cf5a-b9b6bb002dfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input values: [0.5 0.3 0.8]\n",
            "Output values after dense layer with ReLU activation: [0.5451737  0.95509519]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4\n",
        "\n",
        "Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
        "and functionality built into Python).\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 4 -"
      ],
      "metadata": {
        "id": "6XgjON4aqOrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = [[random.random() for _ in range(output_size)] for _ in range(input_size)]\n",
        "        self.biases = [random.random() for _ in range(output_size)]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Perform matrix multiplication and add biases using list comprehensions\n",
        "        weighted_sum = [sum(x * w for x, w in zip(inputs, self.weights[i])) + b for i, b in enumerate(self.biases)]\n",
        "\n",
        "        # Apply activation function (ReLU in this case) using list comprehensions\n",
        "        output = [max(0, x) for x in weighted_sum]\n",
        "        return output"
      ],
      "metadata": {
        "id": "qGPk-ConqHXI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a dense layer with 3 input neurons and 2 output neurons\n",
        "    dense_layer = DenseLayer(input_size=3, output_size=2)\n",
        "\n",
        "    # Provide input values\n",
        "    inputs = [0.5, 0.3, 0.8]\n",
        "\n",
        "    # Forward pass through the dense layer\n",
        "    output = dense_layer.forward(inputs)\n",
        "\n",
        "    # Display the result\n",
        "    print(\"Input values:\", inputs)\n",
        "    print(\"Output values after dense layer with ReLU activation:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsyjRtBdqeYb",
        "outputId": "4265e09c-6432-4a59-bdf3-f1b7b22b2f9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input values: [0.5, 0.3, 0.8]\n",
            "Output values after dense layer with ReLU activation: [0.8585575754586925, 0.3699997105629127]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5\n",
        "\n",
        "What is the “hidden size” of a layer?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 5 -\n",
        "\n",
        "The \"hidden size\" of a layer in a neural network refers to the number of neurons or units in that layer. It is called \"hidden\" because it represents the intermediate computational layer between the input layer and the output layer.\n",
        "\n",
        "In a neural network, information is passed through a series of layers, each layer having a certain number of neurons. The input layer receives the initial data, and the output layer produces the final output or prediction. The layers between the input and output layers are referred to as hidden layers.\n",
        "\n",
        "The hidden size of a layer is a hyperparameter that you can set when designing your neural network. It determines the capacity or complexity of the model. Larger hidden sizes allow the model to learn more complex representations but may also increase the risk of overfitting, especially if the dataset is not large enough.\n",
        "\n",
        "For example, in a dense (fully connected) layer of a neural network, the hidden size corresponds to the number of neurons in that layer. If you have a dense layer with 100 neurons, the hidden size of that layer is 100."
      ],
      "metadata": {
        "id": "C_7OVS5Oqm8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6\n",
        "\n",
        "What does the t method do in PyTorch?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 6 -\n",
        "\n",
        "n PyTorch, the `t method` is used to transpose a tensor. Transposing a tensor means swapping its dimensions. If you have a 2D tensor (matrix), calling t will swap its rows and columns.\n",
        "\n",
        "Here's a simple example:"
      ],
      "metadata": {
        "id": "7vuXQRdcq7EP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor\n",
        "tensor_2d = torch.tensor([[1, 2, 3],\n",
        "                          [4, 5, 6]])\n",
        "\n",
        "# Transpose the tensor\n",
        "transposed_tensor = tensor_2d.t()\n",
        "\n",
        "# Display original and transposed tensors\n",
        "print(\"Original Tensor:\")\n",
        "print(tensor_2d)\n",
        "\n",
        "print(\"\\nTransposed Tensor:\")\n",
        "print(transposed_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJjxEXqkqe2U",
        "outputId": "d757d826-f8b4-485c-850d-e1bbe7a59dfc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "\n",
            "Transposed Tensor:\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7\n",
        "\n",
        "Why is matrix multiplication written in plain Python very slow?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 7 -\n",
        "\n",
        "Matrix multiplication written in plain Python is slow compared to optimized implementations using libraries like NumPy or TensorFlow. There are several reasons for this slowness:\n",
        "\n",
        "1) `Lack of Vectorization` : Plain Python code is not inherently optimized for vectorized operations. Matrix multiplication involves a large number of scalar multiplications and additions, and performing these operations in a loop in Python is much slower than taking advantage of vectorized operations provided by optimized libraries.\n",
        "\n",
        "2) `Interpretation Overhead` : Python is an interpreted language, and the interpretation overhead can be significant for numerical operations. The Python interpreter executes code line by line, which can lead to slower execution compared to compiled languages.\n",
        "\n",
        "3) `Dynamic Typing` : Python is dynamically typed, meaning that variable types are not explicitly declared, and type checking is done at runtime. This flexibility comes at a cost, as it adds extra overhead compared to statically-typed languages like C or Fortran, which can result in slower execution of numerical operations.\n",
        "\n",
        "4) `Memory Management` : Optimized libraries like NumPy use efficient memory management techniques and are often implemented in lower-level languages like C or Fortran, where memory operations can be finely tuned. Plain Python may not have the same level of optimization for memory usage and access patterns.\n",
        "\n",
        "5) `GIL (Global Interpreter Lock)` : In CPython (the reference implementation of Python), the Global Interpreter Lock (GIL) can impact the performance of multi-threaded numerical computations. The GIL ensures that only one thread executes Python bytecode at a time, limiting parallelism in multi-threaded programs.\n",
        "\n",
        "To overcome these issues and achieve better performance, it's recommended to use libraries like NumPy, which is implemented in C and optimized for numerical operations. NumPy leverages efficient algorithms, parallel processing, and optimized memory management, leading to significantly faster matrix multiplication compared to plain Python implementations. Additionally, using specialized numerical libraries like TensorFlow or PyTorch can further accelerate matrix operations by leveraging GPU acceleration for large-scale computations."
      ],
      "metadata": {
        "id": "G7dng9W7ri10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8\n",
        "\n",
        "In matmul, why is ac==br?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 8 -\n",
        "\n",
        "In the context of matrix multiplication, the requirement that `ac == br` refers to the dimensions of the matrices involved in the multiplication operation.\n",
        "\n",
        "Let's consider two matrices:\n",
        "\n",
        "1) A matrix of size `(a, b)` denoted as `A` , where `a` is the number of `rows` and `b` is the number of `columns` .\n",
        "\n",
        "2) Another matrix of size `(c, d)` denoted as `B` , where `c` is the number of `rows` and `d` is the number of `columns` .\n",
        "\n",
        "The product of matrices `A` and `B` is given by the matrix `C` with dimensions `(ac, bd)` . To ensure that the multiplication is valid, it must satisfy the condition `ac == br` , where:\n",
        "\n",
        "-`ac` is the number of columns in matrix `A` (which is `b`).\n",
        "\n",
        "- `br` is the number of rows in matrix `B` (which is also `c`).\n",
        "\n",
        "Therefore, for the multiplication `A @ B` to be valid, the number of columns in `A` must be equal to the number of rows in `B` . This condition ensures that the inner dimensions match, allowing the matrix multiplication to proceed and resulting in a matrix with dimensions `(a, d)` .\n",
        "\n",
        "In Python, when using the `@`  operator for matrix multiplication (or the `np.matmul` function in NumPy), the dimensions are automatically checked, and a ValueError will be raised if the condition `ac == br` is not satisfied. This helps catch potential errors in matrix dimensions before attempting the multiplication."
      ],
      "metadata": {
        "id": "TbqQv2Fbr6tO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9\n",
        "\n",
        "In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 9 -\n",
        "\n",
        "In Jupyter Notebook, you can use the %time or %timeit magic commands to measure the time taken for the execution of a single cell.\n",
        "\n",
        "1) **%time Magic Command**\n",
        "\n",
        "- `%time` provides a simple way to measure the elapsed time for the execution of a single cell.\n",
        "\n",
        "- Place `%time` at the beginning of the cell, followed by the code you want to measure."
      ],
      "metadata": {
        "id": "HNwZ4tEJtHtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "3Ek_p4pYrK2v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) **%timeit Magic Command**\n",
        "\n",
        "- `%timeit` is more sophisticated than %time and performs multiple runs to get a more accurate estimate.\n",
        "\n",
        "- It provides additional statistical information such as mean and standard deviation.\n",
        "\n",
        "- Place %timeit at the beginning of the cell, followed by the code you want to measure."
      ],
      "metadata": {
        "id": "4zUnJqC2zD-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "LeywgWs2zAzz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example:"
      ],
      "metadata": {
        "id": "aCWDimSfzba2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Using %time\n",
        "%time sum(range(1000000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EeyAFp8zRXE",
        "outputId": "c13aa92b-5299-4f7d-fd5f-7cd0e603250e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.7 ms, sys: 789 µs, total: 21.5 ms\n",
            "Wall time: 21.4 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "499999500000"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Using %timeit\n",
        "%timeit sum(range(1000000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6aXaxdfzfDn",
        "outputId": "6d3a321a-7ad5-4d66-81c3-c5721d3c63bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.7 ms ± 3.91 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10\n",
        "\n",
        "What is elementwise arithmetic?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 10 -\n",
        "\n",
        "Elementwise arithmetic refers to performing arithmetic operations on the corresponding elements of two or more arrays or matrices. In elementwise arithmetic, each element of one array is paired with the corresponding element of another array, and the arithmetic operation is performed on those pairs of elements.\n",
        "\n",
        "For example, given two arrays A and B, elementwise addition would involve adding each element of A to the corresponding element of B, resulting in a new array C where each element is the sum of the corresponding elements in A and B.\n",
        "\n",
        "Elementwise arithmetic is commonly used in numerical computations, particularly in linear algebra and machine learning. In Python, libraries such as NumPy provide efficient support for elementwise arithmetic operations on arrays and matrices.\n",
        "\n",
        "Elementwise arithmetic is different from matrix multiplication, where each element in the resulting matrix is the sum of the products of the corresponding elements of two input matrices. In matrix multiplication, the input matrices must have compatible shapes, whereas in elementwise arithmetic, the input arrays must have the same shape."
      ],
      "metadata": {
        "id": "t-TE0c9Szvop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 11\n",
        "\n",
        "Write the PyTorch code to test whether every element of a is greater than the\n",
        "corresponding element of b.\n",
        "\n",
        ".................\n",
        "\n",
        "Answer 11 -"
      ],
      "metadata": {
        "id": "6Y-dtDcIz5id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sample tensors a and b\n",
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([0, 2, 2])\n",
        "\n",
        "# Test whether every element of a is greater than the corresponding element of b\n",
        "result = torch.all(a > b)\n",
        "\n",
        "# Display the result\n",
        "print(\"Every element of 'a' is greater than the corresponding element of 'b':\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4MHne5XznCu",
        "outputId": "b2aafe22-04ea-42b8-b7f2-633e64bf4e67"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every element of 'a' is greater than the corresponding element of 'b': tensor(False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 12\n",
        "\n",
        "What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 12 -\n",
        "\n",
        "A rank-0 tensor is essentially a scalar, meaning it has zero dimensions. It represents a single numerical value and doesn't have any axes or dimensions. In PyTorch, rank-0 tensors are created using `torch.tensor()` or `torch.scalar_tensor()` .\n",
        "\n",
        "To convert a rank-0 tensor (scalar) to a plain Python data type, you can use the `.item()` method. This method extracts the single numerical value from the tensor and returns it as a native Python scalar.\n",
        "\n",
        "Here's an example:"
      ],
      "metadata": {
        "id": "rte55uGh2lMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Creating a rank-0 tensor (scalar)\n",
        "scalar_tensor = torch.tensor(42)\n",
        "\n",
        "# Converting the rank-0 tensor to a plain Python data type\n",
        "scalar_value = scalar_tensor.item()\n",
        "\n",
        "# Displaying the result\n",
        "print(\"Rank-0 Tensor (Scalar):\", scalar_tensor)\n",
        "print(\"Converted to Python Scalar:\", scalar_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk44-rSq2VTQ",
        "outputId": "4fd5fd7d-bed6-46d3-bd09-1f21567fb387"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank-0 Tensor (Scalar): tensor(42)\n",
            "Converted to Python Scalar: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 13\n",
        "\n",
        "How does elementwise arithmetic help us speed up matmul?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 13 -\n",
        "\n",
        "Elementwise arithmetic does not directly speed up matrix multiplication (matmul), but it is a fundamental concept in understanding how optimized numerical libraries, such as NumPy or TensorFlow, can achieve faster matrix operations. Let me explain the relationship between elementwise arithmetic and matrix multiplication optimization:\n",
        "\n",
        "1) `Vectorization` :\n",
        "\n",
        "- Elementwise arithmetic operations can be vectorized, meaning they can be applied to entire arrays or vectors at once, avoiding the need for explicit loops.\n",
        "\n",
        "- Vectorized operations take advantage of low-level optimizations implemented in numerical libraries, which often use highly efficient, parallelized implementations written in languages like C or Fortran.\n",
        "\n",
        "2) `Optimized BLAS Libraries` :\n",
        "\n",
        "- Many numerical libraries, including NumPy and TensorFlow, use optimized Basic Linear Algebra Subprograms (BLAS) libraries under the hood.\n",
        "\n",
        "- BLAS libraries are highly optimized and are designed to perform efficient linear algebra operations, including matrix multiplication, using low-level, hardware-specific optimizations.\n",
        "\n",
        "3) `Strided Memory Access` :\n",
        "\n",
        "- Optimized matrix multiplication algorithms leverage strided memory access patterns to efficiently access and process data in memory.\n",
        "\n",
        "- Elementwise operations contribute to creating strided patterns, as they operate on contiguous blocks of memory.\n",
        "\n",
        "4) `Parallelism` :\n",
        "\n",
        "- Modern hardware, including multi-core CPUs and GPUs, supports parallel processing.\n",
        "\n",
        "- Elementwise operations, when applied to large arrays, can be parallelized to take advantage of the available parallel processing capabilities."
      ],
      "metadata": {
        "id": "bLjORmbF2649"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 14\n",
        "\n",
        "What are the broadcasting rules?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 14 -\n",
        "\n",
        "Broadcasting is a mechanism in NumPy (and other similar libraries) that allows for performing elementwise operations on arrays of different shapes and sizes. The broadcasting rules define how these operations are carried out when the shapes of the input arrays are not identical.\n",
        "\n",
        "Here are the broadcasting rules:\n",
        "\n",
        "1) `Dimensions Compatibility` : If the arrays have a different number of dimensions, pad the smaller shape with ones on its left side until the sizes match.\n",
        "\n",
        "- Example: If one array has shape (3, 1, 6) and the other has shape (5, 6), pad the first array to (1, 3, 1, 6).\n",
        "\n",
        "2) `Size Compatibility` : For each dimension, the sizes must either be equal, or one of them must be 1.\n",
        "\n",
        "- Example: Arrays with shapes (5, 1, 3) and (1, 4, 3) are compatible because the sizes along each dimension are either equal or one of them is 1.\n",
        "\n",
        "3) `Broadcasting` : Once the sizes are compatible, NumPy will perform elementwise operations by broadcasting the smaller array along the dimensions where its size is 1 to match the corresponding size in the larger array.\n",
        "\n",
        "- Example: Arrays with shapes (3, 1, 6) and (5, 1, 6) are compatible, and broadcasting will be performed along the second dimension.\n",
        "\n",
        "4) `Size of Output` : The size of the output array is determined by taking the maximum size along each dimension.\n",
        "\n",
        "- Example: If the input arrays have shapes (3, 1, 6) and (5, 1, 6), the output shape will be (5, 3, 6).\n",
        "\n",
        "Here's an example to illustrate these rules:"
      ],
      "metadata": {
        "id": "-z-X2EUR3bbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example arrays\n",
        "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "B = np.array([10, 20, 30])\n",
        "\n",
        "# Broadcasting: B is broadcasted to match the shape of A\n",
        "result = A + B\n",
        "\n",
        "# Display the result\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hFU6CLg20l6",
        "outputId": "ea78dba0-4385-44c5-a88e-a67a8af18d03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11 22 33]\n",
            " [14 25 36]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 15\n",
        "\n",
        "What is expand_as? Show an example of how it can be used to match the results of\n",
        "broadcasting.\n",
        "\n",
        "................\n",
        "\n",
        "Answer 15 -\n",
        "\n",
        "In PyTorch, `expand_as` is a method that is used to expand the size of one tensor to match the size of another tensor. It is commonly used to ensure that two tensors have the same size or shape before performing elementwise operations, similar to broadcasting in NumPy.\n",
        "\n",
        "Here's an example of how `expand_as` can be used to match the results of broadcasting:"
      ],
      "metadata": {
        "id": "65FQGvGL4Dlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example tensors\n",
        "tensor_A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_B = torch.tensor([10, 20, 30])\n",
        "\n",
        "# Broadcasting without matching shapes\n",
        "result_broadcasting = tensor_A + tensor_B\n",
        "\n",
        "# Using expand_as to match shapes before addition\n",
        "tensor_B_expanded = tensor_B.expand_as(tensor_A)\n",
        "result_expanded = tensor_A + tensor_B_expanded\n",
        "\n",
        "# Display the results\n",
        "print(\"Result with broadcasting:\\n\", result_broadcasting)\n",
        "print(\"\\nResult with expand_as:\\n\", result_expanded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23EGC7b_3nu9",
        "outputId": "d4549820-df68-49be-cc20-fa3e1752de5c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result with broadcasting:\n",
            " tensor([[11, 22, 33],\n",
            "        [14, 25, 36]])\n",
            "\n",
            "Result with expand_as:\n",
            " tensor([[11, 22, 33],\n",
            "        [14, 25, 36]])\n"
          ]
        }
      ]
    }
  ]
}