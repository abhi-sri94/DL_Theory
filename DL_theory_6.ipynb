{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6uZ8mxrZmgg"
      },
      "source": [
        "#Question 1\n",
        "\n",
        "What are the advantages of a CNN over a fully connected DNN for image classification?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 1 -\n",
        "\n",
        "Convolutional Neural Networks (CNNs) offer several advantages over fully connected Deep Neural Networks (DNNs) for image classification tasks:\n",
        "\n",
        "Some of them are below:\n",
        "\n",
        "1) **Local Receptive Fields** : CNNs are designed to recognize patterns in images by using local receptive fields, which allows them to focus on small, local features like edges, textures, and shapes. In contrast, fully connected DNNs treat every input neuron as connected to every neuron in the subsequent layer, making them less efficient at capturing local patterns.\n",
        "\n",
        "2) **Parameter Sharing** : CNNs use weight sharing, where the same set of weights (filters) is applied to different parts of the input image. This sharing of parameters reduces the number of learnable parameters, making CNNs more parameter-efficient compared to fully connected DNNs, especially for large images.\n",
        "\n",
        "3) **Translation Invariance** : CNNs are naturally suited for recognizing objects regardless of their position in the image. This translation invariance is achieved through pooling layers that downsample feature maps. In contrast, fully connected DNNs are sensitive to the precise spatial arrangement of pixels, which makes them less suitable for recognizing objects in different positions.\n",
        "\n",
        "4) **Hierarchical Feature Learning** : CNN architectures typically consist of multiple layers with increasing abstraction. Lower layers capture simple features like edges and textures, while higher layers learn complex features and object representations. This hierarchical feature learning is well-suited for image classification tasks.\n",
        "\n",
        "5) **Reduced Overfitting** : CNNs are less prone to overfitting compared to fully connected DNNs because of their parameter sharing and local feature extraction. This property is especially valuable when working with limited training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD7AdX6EWMqA"
      },
      "source": [
        "#Question 2\n",
        "\n",
        "Consider a CNN composed of three convolutional layers, each with 3 x 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs\n",
        "200, and the top one outputs 400. The input images are RGB images of 200 x 300 pixels.\n",
        "\n",
        "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how muchRAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
        "\n",
        ".................\n",
        "\n",
        "Answer 2 -\n",
        "\n",
        "To calculate the total number of parameters in the CNN, we need to count the number of parameters in each layer and then sum them up. The number of parameters in a convolutional layer depends on the number of filters, the size of the filters, and the number of input channels. The formula for calculating the number of parameters in a convolutional layer is:\n",
        "\n",
        "`number of parameters = (size of filter * number of input channels + 1) * number of filters`\n",
        "\n",
        "Using the given information, we can calculate the number of parameters in each convolutional layer:\n",
        "\n",
        "- The first convolutional layer has 3x3x3x100 + 100 = 2,800 parameters\n",
        "\n",
        "- The second convolutional layer has 3x3x100x200 + 200 = 180,200 parameters\n",
        "\n",
        "- The third convolutional layer has 3x3x200x400 + 400 = 1,160,400 parameters\n",
        "\n",
        "Therefore, the total number of parameters in the CNN is 2,800 + 180,200 + 1,160,400 = 1,343,400 parameters.\n",
        "\n",
        "To calculate the amount of RAM required for prediction or training, we need to consider the size of the input and output tensors and the data type being used. Assuming we are using 32-bit floats (4 bytes), the size of the input tensor for a single image is 200x300x3 = 180,000 bytes. The size of the output tensor for a single image is (200/8)x(300/8)x400x4 = 6,000,000 bytes. Therefore, the total RAM required for prediction for a single instance is approximately 6,180,000 bytes (6.18 MB).\n",
        "\n",
        "If we are training on a mini-batch of 50 images, the total RAM required would be 50 times the RAM required for a single instance, which is approximately 309 MB. However, this is just the memory required for storing the input and output tensors during one forward pass of the network. The actual amount of memory required for training will depend on the batch size, the size of the model, the optimizer being used, and other factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVlvazwOdyPx"
      },
      "source": [
        "#Question 3\n",
        "\n",
        "If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 3 -\n",
        "\n",
        "When your GPU runs out of memory while training a Convolutional Neural Network (CNN), it can be challenging, but there are several strategies you can try to address the issue and continue training effectively:\n",
        "\n",
        "1) **Reduce Batch Size** : Decrease the batch size used during training. Smaller batch sizes require less GPU memory. However, too small a batch size can slow down training due to increased data transfer overhead.\n",
        "\n",
        "2) **Gradient Accumulation** : Implement gradient accumulation, where you accumulate gradients over multiple smaller batches before applying weight updates. This effectively simulates a larger batch size without increasing memory usage.\n",
        "\n",
        "3) **Reduce Model Complexity** : Simplify your model architecture by reducing the number of layers, the number of neurons in each layer, or the number of parameters. Smaller models require less memory.\n",
        "\n",
        "4) **Use Mixed Precision Training** : Utilize mixed precision training, which combines 16-bit floating-point numbers for activations and gradients with 32-bit floating-point numbers for model weights. This reduces memory usage without significant loss of training quality.\n",
        "\n",
        "5) **Reduce Input Image Size** : Resize input images to a smaller resolution before feeding them into the network. Smaller images require less GPU memory but may impact model performance.\n",
        "\n",
        "6) **Utilize GPU with More Memory** : If possible, switch to a GPU with more memory capacity. Larger GPUs can handle larger models and batch sizes, reducing the likelihood of memory issues.\n",
        "\n",
        "7) **Use Model Parallelism** : Split the model across multiple GPUs, with each GPU responsible for a subset of the layers. This allows you to train larger models without increasing the memory load on a single GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woA_tRvCe-Oj"
      },
      "source": [
        "#Quetsion 4\n",
        "\n",
        "Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
        "\n",
        "................\n",
        "\n",
        "Answer 4 -\n",
        "\n",
        "Adding a Max Pooling layer instead of a Convolutional layer with the same stride serves specific purposes in Convolutional Neural Networks (CNNs) and contributes to different aspects of the network's performance and representational capabilities. Here are some reasons why you might want to use a Max Pooling layer:\n",
        "\n",
        "1) **Downsampling and Spatial Hierarchies** :\n",
        "\n",
        "- Max Pooling is primarily used for downsampling the spatial dimensions of the feature maps. By selecting the maximum value within each pooling region, it retains the most dominant features while reducing the spatial resolution.\n",
        "\n",
        "- Downsampling helps create a spatial hierarchy of features in the network, allowing the network to focus on larger and more abstract features in higher layers. This is essential for recognizing objects at different scales.\n",
        "\n",
        "2) **Translation Invariance** :\n",
        "\n",
        "- Max Pooling enhances the network's ability to achieve translation invariance, meaning it can recognize patterns or features regardless of their exact position in the input. By pooling the maximum value in a local region, the network becomes less sensitive to small translations in the input image.\n",
        "\n",
        "3) **Dimensionality Reduction** :\n",
        "\n",
        "- Max Pooling reduces the dimensionality of the feature maps, which can help control computational complexity, reduce memory usage, and mitigate overfitting. Smaller feature maps are computationally less expensive to process in subsequent layers.\n",
        "\n",
        "4) **Non-linearity and Robustness** :\n",
        "\n",
        "- Max Pooling introduces a non-linear element into the network, as it selects the maximum value from each pooling region. This non-linearity can make the network more robust to variations in the input.\n",
        "\n",
        "5) **Parameter Efficiency** :\n",
        "\n",
        "- Max Pooling requires fewer learnable parameters compared to Convolutional layers. Convolutional layers have weights and biases that need to be trained, while Max Pooling is parameter-free.\n",
        "\n",
        "6) **Interpretable Features** :\n",
        "\n",
        "- Max Pooling retains the most dominant features within each pooling region, making the extracted features somewhat interpretable. These features often correspond to specific textures or patterns.\n",
        "\n",
        "7) **Reduction of Spatial Information** :\n",
        "\n",
        "- In certain cases, you may want to reduce the spatial information to focus on high-level semantics. Max Pooling helps remove fine-grained spatial details, which can be useful when the exact spatial location of features is less important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2V7LfX7f7Ov"
      },
      "source": [
        "#Question 5\n",
        "\n",
        "When would you want to add a local response normalization layer?\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 5 -\n",
        "\n",
        "Local Response Normalization (LRN) layers were once popular in Convolutional Neural Networks (CNNs), but they have become less commonly used in modern architectures. LRN layers were originally introduced in AlexNet, which won the ImageNet Large Scale Visual Recognition Challenge in 2012. LRN was designed to provide local contrast normalization, which can enhance the generalization ability of a network. However, there are specific scenarios where you might consider adding an LRN layer:\n",
        "\n",
        "1) **Historical Models** : If you are working with older CNN architectures like AlexNet or early versions of GoogleNet (Inception), you might encounter LRN layers. In these cases, you would add LRN layers to replicate the original architecture.\n",
        "\n",
        "2) **Network Interpretability** : In some cases, you may want to add LRN layers to interpret and analyze how the network responds to local contrast normalization. This can help gain insights into feature responses and their impact on the network's decisions.\n",
        "\n",
        "3) **Reproducing Research** : If you are trying to reproduce research results from older papers that used LRN layers, you would include them for consistency.\n",
        "\n",
        "4) **Custom Architectures** : In rare cases, you may have a specific problem where local contrast normalization is beneficial, and you decide to design a custom architecture that includes LRN layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwP_iUdDL1Jd"
      },
      "source": [
        "#Question 6\n",
        "\n",
        "Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 6 -\n",
        "\n",
        "Here are the main innovations in each of the mentioned neural network architectures compared to their predecessors:\n",
        "\n",
        "1) **AlexNet (2012) vs. LeNet-5 (1998)** :\n",
        "\n",
        "- `Deep Architecture` : AlexNet introduced a much deeper architecture compared to LeNet-5. It had eight layers, including five convolutional layers and three fully connected layers, which made it significantly deeper than LeNet-5's architecture.\n",
        "\n",
        "- `ReLU Activation` : AlexNet used the Rectified Linear Unit (ReLU) activation function, which helped mitigate the vanishing gradient problem and enabled faster training compared to LeNet-5's sigmoid activation.\n",
        "\n",
        "- `Local Response Normalization (LRN)` : AlexNet incorporated LRN layers, which provided local contrast normalization to the network's activations, promoting better generalization.\n",
        "\n",
        "- `Data Augmentation` : AlexNet used data augmentation techniques like random cropping and flipping during training, which helped improve the model's robustness.\n",
        "\n",
        "2) **GoogLeNet (Inception, 2014)** :\n",
        "\n",
        "- `Inception Modules` : The Inception architecture introduced the concept of Inception modules, which allowed the network to capture features at multiple scales by using different filter sizes within a single layer.\n",
        "\n",
        "- `Network Depth` : GoogLeNet increased network depth by using a large number of layers without an exponential increase in computational cost. This was achieved through the efficient use of Inception modules.\n",
        "\n",
        "- `Global Average Pooling` : Instead of fully connected layers, GoogLeNet used global average pooling, reducing the number of parameters and enabling the model to be more invariant to translation.\n",
        "\n",
        "3) **ResNet (2015)** :\n",
        "\n",
        "- `Residual Connections` : ResNet introduced residual connections, allowing information to flow more easily through the network by skipping certain layers. This innovation enabled training of extremely deep networks, with hundreds of layers.\n",
        "\n",
        "- `Very Deep Architectures` : ResNet demonstrated the effectiveness of very deep architectures, with models containing hundreds of layers that outperformed shallower networks.\n",
        "\n",
        "4) **SENet (Squeeze-and-Excitation Networks, 2017)** :\n",
        "\n",
        "- `Squeeze-and-Excitation Blocks` : SENet introduced squeeze-and-excitation blocks that adaptively recalibrate the channel-wise feature responses in the network. This helped the network focus on informative channels while suppressing less useful ones, improving feature representation.\n",
        "\n",
        "5) **Xception (Extreme Inception, 2017)** :\n",
        "\n",
        "- `Depthwise Separable Convolutions` : Xception replaced standard convolutions with depthwise separable convolutions, which significantly reduced the number of parameters while preserving representational power.\n",
        "\n",
        "- `Separation of Spatial and Channel-wise Operations` : Xception separated spatial and channel-wise operations, allowing efficient feature extraction and increasing network efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJUyJfUnNvBB"
      },
      "source": [
        "#Question 7\n",
        "What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 7 -\n",
        "\n",
        "A Fully Convolutional Network (FCN) is a type of neural network architecture designed for pixel-wise prediction tasks, such as image segmentation, where the goal is to classify each pixel in an input image. Unlike traditional Convolutional Neural Networks (CNNs) that consist of convolutional and fully connected layers, FCNs replace the fully connected layers with convolutional layers to maintain spatial information throughout the network.\n",
        "\n",
        "Here's how you can convert a dense (fully connected) layer into a convolutional layer:\n",
        "\n",
        "1) **Change the Layer Type** :\n",
        "\n",
        "- Replace the dense layer in your neural network architecture with a convolutional layer. This means replacing `tf.keras.layers.Dense` with `tf.keras.layers.Conv2D` in TensorFlow or the equivalent layers in other deep learning frameworks.\n",
        "\n",
        "2) **Adjust the Parameters** :\n",
        "\n",
        "When converting a dense layer to a convolutional layer, you need to adjust the following parameters:\n",
        "\n",
        "a) `Filters/Units` : The number of filters (neurons) in the convolutional layer should match the number of units in the dense layer.\n",
        "\n",
        "b) `Kernel Size` : Specify the size of the convolutional kernel. For example, a 1x1 kernel size is equivalent to a fully connected layer.\n",
        "\n",
        "c) `Stride` : Set the stride to 1 for a 1x1 convolution, which is equivalent to connecting each output neuron to every input neuron in the previous layer.\n",
        "\n",
        "d) `Padding` : Typically, use \"valid\" padding to ensure that the output size matches the size of the original dense layer's output.\n",
        "\n",
        "Here's an example in TensorFlow of converting a dense layer to a convolutional layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fQ89YluOpoB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Original Dense Layer\n",
        "dense_layer = tf.keras.layers.Dense(units=256, activation='relu')\n",
        "\n",
        "# Equivalent Convolutional Layer\n",
        "conv_layer = tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1), activation='relu', strides=(1, 1), padding='valid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aDmjU_xZALM"
      },
      "source": [
        "#Question 8\n",
        "\n",
        "What is the main technical difficulty of semantic segmentation?\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 8\n",
        "\n",
        "The main technical difficulty of semantic segmentation is the challenge of achieving pixel-level classification of objects or regions in an image accurately and efficiently. Semantic segmentation involves assigning a semantic label to each pixel in an image to identify the object or category it belongs to. The primary difficulties associated with semantic segmentation are as follows:\n",
        "\n",
        "1) **Pixel-wise Classification** : Semantic segmentation requires making predictions for every pixel in an image, which is computationally intensive and memory-consuming, especially for high-resolution images. Managing the sheer number of pixels in an image is a technical challenge.\n",
        "\n",
        "2) **Object and Instance Differentiation** : Distinguishing between different objects and instances of the same object within an image can be challenging. For example, distinguishing between different cars or different people in a scene.\n",
        "\n",
        "3) **Semantic Ambiguity** : In some cases, there may be semantic ambiguity where a pixel could belong to multiple classes or object categories simultaneously. Handling such ambiguity is a complex problem.\n",
        "\n",
        "4) **Object Occlusion** : Objects in real-world images are often partially occluded by other objects or obstructions. Segmenting objects accurately in the presence of occlusion is a challenging task.\n",
        "\n",
        "5) **Variability in Object Appearance** : Objects can appear in various orientations, scales, lighting conditions, and poses. The model needs to generalize well to handle this variability.\n",
        "\n",
        "6) **Sparse Object Instances** : In some scenarios, objects of interest may be sparse in the image, making it difficult for the model to identify and segment them accurately.\n",
        "\n",
        "7) **Real-time Processing** : For applications like autonomous driving or robotics, real-time semantic segmentation is required, which demands fast inference and low-latency models.\n",
        "\n",
        "8) **Data Annotation** : Creating high-quality pixel-level annotations for training datasets is time-consuming and expensive. Annotating large datasets with accurate segmentation masks is a bottleneck.\n",
        "\n",
        "9) **Model Complexity** : Achieving high segmentation accuracy often requires using complex neural network architectures, which are computationally expensive and challenging to train.\n",
        "\n",
        "10) **Class Imbalance** : Imbalanced class distributions in the training data, where some classes are more prevalent than others, can lead to biased models that perform poorly on underrepresented classes.\n",
        "\n",
        "11) **Memory Constraints** : Memory constraints on GPUs limit the size of input images and the complexity of the models that can be used for semantic segmentation tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH4QEeVnazOa"
      },
      "source": [
        "#Question 9\n",
        "\n",
        "Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 9\n",
        "\n",
        "Certainly, I can provide you with a simple CNN architecture in Python using TensorFlow/Keras to achieve high accuracy on the MNIST dataset. The MNIST dataset contains hand-written digits (0-9), and it's a common benchmark for image classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejC0Tk_QcUFZ",
        "outputId": "610a7373-4054-4b51-906f-d0dc07da0318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.2009 - accuracy: 0.9385 - val_loss: 0.0426 - val_accuracy: 0.9867\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0732 - accuracy: 0.9783 - val_loss: 0.0303 - val_accuracy: 0.9895\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0548 - accuracy: 0.9834 - val_loss: 0.0287 - val_accuracy: 0.9903\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0443 - accuracy: 0.9865 - val_loss: 0.0270 - val_accuracy: 0.9898\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.0241 - val_accuracy: 0.9919\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 0.0219 - val_accuracy: 0.9933\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0275 - accuracy: 0.9911 - val_loss: 0.0264 - val_accuracy: 0.9920\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0246 - val_accuracy: 0.9929\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.0229 - val_accuracy: 0.9936\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0183 - accuracy: 0.9937 - val_loss: 0.0255 - val_accuracy: 0.9925\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values to the range [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add a channel dimension to the images\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xE5KD9wQZo3",
        "outputId": "a410d9d2-eda4-4b08-f0a5-2a526244ef65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - loss: 0.0255 - accuracy: 0.9925 - 2s/epoch - 7ms/step\n",
            "\n",
            "Test accuracy: 0.9925000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkL8rPeDfBXS"
      },
      "source": [
        "#Question 10\n",
        "\n",
        "Use transfer learning for large image classification, going through these steps:\n",
        "\n",
        "a) Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
        "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
        "\n",
        "b) Split it into a training set, a validation set, and a test set.\n",
        "\n",
        "c) Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.\n",
        "\n",
        "d) Fine-tune a pretrained model on this dataset.\n",
        "\n",
        ".................\n",
        "\n",
        "Answer 10\n",
        "\n",
        "a) Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_crop(image, size=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess_image).shuffle(1000).batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(preprocess_image).batch(BATCH_SIZE)\n",
        "test_ds = test_ds.map(preprocess_image).batch(BATCH_SIZE)\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds, epochs=5, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z69xTxsJ-sB",
        "outputId": "346eca5a-e094-4dd2-bec9-144517cad151"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 4s 0us/step\n",
            "Epoch 1/5\n",
            "582/582 [==============================] - 2353s 4s/step - loss: 0.0532 - accuracy: 0.9813 - val_loss: 0.0367 - val_accuracy: 0.9888\n",
            "Epoch 2/5\n",
            "582/582 [==============================] - 2321s 4s/step - loss: 0.0362 - accuracy: 0.9881 - val_loss: 0.0612 - val_accuracy: 0.9828\n",
            "Epoch 3/5\n",
            "582/582 [==============================] - 2339s 4s/step - loss: 0.0327 - accuracy: 0.9891 - val_loss: 0.0434 - val_accuracy: 0.9854\n",
            "Epoch 4/5\n",
            "582/582 [==============================] - 2302s 4s/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 0.0510 - val_accuracy: 0.9845\n",
            "Epoch 5/5\n",
            "582/582 [==============================] - 2248s 4s/step - loss: 0.0275 - accuracy: 0.9912 - val_loss: 0.0387 - val_accuracy: 0.9888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrWrr0-rQreB",
        "outputId": "515747da-4b8d-4689-b54e-d6a25574650e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 264s 4s/step - loss: 0.0304 - accuracy: 0.9893\n",
            "Test accuracy: 0.9892519116401672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) **Split it into a training set, a validation set, and a test set.**"
      ],
      "metadata": {
        "id": "a3Cwxf4I_pJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                     with_info=True,\n",
        "                     as_supervised=True)"
      ],
      "metadata": {
        "id": "jZEFDIuE95dO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) **Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.**"
      ],
      "metadata": {
        "id": "qhD4TkJFAKEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load the dataset\n",
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)\n",
        "\n",
        "# Define preprocessing functions\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "\n",
        "    # Resize the image to the input size of the model\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Convert the pixel values to the range [0, 1]\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "def augment_image(image, label):\n",
        "\n",
        "    # Randomly flip the image horizontally\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # Randomly adjust the brightness of the image\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    return image, label\n",
        "\n",
        "# Create the training dataset\n",
        "train_ds = train_ds.shuffle(10000)\n",
        "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.batch(batch_size=32)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create the validation dataset\n",
        "val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(batch_size=32)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Create the test dataset\n",
        "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size=32)\n",
        "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "B0Ho3R8u_5P2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) **Fine-tune a pretrained model on this dataset.**"
      ],
      "metadata": {
        "id": "zWoDs81GBIuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "# Load the dataset\n",
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)\n",
        "\n",
        "# Define preprocessing functions\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "\n",
        "    # Resize the image to the input size of the model\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Convert the pixel values to the range [-1, 1]\n",
        "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing to the datasets\n",
        "train_ds = train_ds.map(preprocess_image).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_image).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess_image).batch(32)\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model\n",
        "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze the base model's layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add a new classification layer on top of the base model\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(global_average_layer)\n",
        "\n",
        "# Create the fine-tuned model\n",
        "model = tf.keras.models.Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training set\n",
        "history = model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmSDi_nvAo3o",
        "outputId": "b61a0d52-5601-4e30-8e60-12af8186a0e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            "582/582 [==============================] - 846s 1s/step - loss: 0.0508 - accuracy: 0.9831 - val_loss: 0.0353 - val_accuracy: 0.9871\n",
            "Epoch 2/10\n",
            "582/582 [==============================] - 815s 1s/step - loss: 0.0293 - accuracy: 0.9896 - val_loss: 0.0366 - val_accuracy: 0.9875\n",
            "Epoch 3/10\n",
            "582/582 [==============================] - 784s 1s/step - loss: 0.0250 - accuracy: 0.9912 - val_loss: 0.0356 - val_accuracy: 0.9884\n",
            "Epoch 4/10\n",
            "582/582 [==============================] - 698s 1s/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.0371 - val_accuracy: 0.9875\n",
            "Epoch 5/10\n",
            "582/582 [==============================] - 709s 1s/step - loss: 0.0183 - accuracy: 0.9936 - val_loss: 0.0380 - val_accuracy: 0.9880\n",
            "Epoch 6/10\n",
            "582/582 [==============================] - 703s 1s/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.0387 - val_accuracy: 0.9893\n",
            "Epoch 7/10\n",
            "582/582 [==============================] - 701s 1s/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.0414 - val_accuracy: 0.9871\n",
            "Epoch 8/10\n",
            "582/582 [==============================] - 766s 1s/step - loss: 0.0136 - accuracy: 0.9963 - val_loss: 0.0420 - val_accuracy: 0.9875\n",
            "Epoch 9/10\n",
            "582/582 [==============================] - 781s 1s/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0420 - val_accuracy: 0.9888\n",
            "Epoch 10/10\n",
            "582/582 [==============================] - 793s 1s/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.0439 - val_accuracy: 0.9867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv_jGeayBG2D",
        "outputId": "25c128a0-c89e-4ea9-db1f-bda4afadf3ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 88s 1s/step - loss: 0.0253 - accuracy: 0.9923\n",
            "Test loss: 0.025340871885418892, Test accuracy: 0.9922614097595215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09bAl1QpfFet"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}