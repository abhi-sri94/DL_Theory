{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1\n",
        "\n",
        "Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 1 -\n",
        "\n",
        "Initializing all the weights to the same value, even if that value is selected randomly using He initialization, is not recommended for deep neural networks. While He initialization is a good choice to ensure that the weights are initialized to appropriate values, the key idea is to introduce diversity and break symmetry among the neurons. Initializing all weights to the same value would not achieve this diversity.\n",
        "\n",
        "Here's why initializing all weights to the same value is not ideal:\n",
        "\n",
        "1) **Symmetry Breaking** : The purpose of weight initialization is to break the symmetry among neurons in the same layer. If all weights are initialized to the same value, even a random value, it effectively means that the neurons are initially identical, which can lead to them learning the same features and not contributing diversity to the network.\n",
        "\n",
        "2) **Diversity of Neurons** : In a deep neural network, you want each neuron to learn different features or representations from the data. This diversity helps the network capture complex patterns and relationships. Initializing weights with different random values helps in achieving this diversity.\n",
        "\n",
        "3) **Avoiding Vanishing Gradients** : He initialization specifically scales the weights based on the number of input units to the neuron. This scaling helps in avoiding the vanishing gradient problem during training. If all weights are set to the same value, this scaling would be lost, potentially leading to convergence issues.\n",
        "\n",
        "4) **Improved Training** : Diverse weight initialization encourages neurons to start learning different features from the beginning of training. This can lead to faster convergence and better generalization."
      ],
      "metadata": {
        "id": "Pcjnx1otJow-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2\n",
        "\n",
        "Is it OK to initialize the bias terms to 0?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 2 -\n",
        "\n",
        "Yes, it is generally acceptable to initialize the bias terms to 0. However, it is important to note that other values may result in better performance, depending on the type of model and data."
      ],
      "metadata": {
        "id": "aWbL-0pCPgWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3\n",
        "\n",
        "Name three advantages of the SELU activation function over ReLU.\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 3 -\n",
        "\n",
        "The Scaled Exponential Linear Unit (SELU) activation function offers several advantages over the Rectified Linear Unit (ReLU) activation function. Here are three key advantages of SELU:\n",
        "\n",
        "1) **Self-Normalization** : SELU activations are designed to promote self-normalization in deep neural networks. This means that in a properly configured network, the activations tend to converge toward a mean of 0 and a standard deviation of 1 during training. This self-normalizing property mitigates the vanishing and exploding gradient problems, making it easier to train very deep networks.\n",
        "\n",
        "2) **Avoidance of Dead Neurons** : SELU activations are less prone to the \"dying ReLU\" problem, which occurs when neurons become inactive and never activate again during training. The dying ReLU problem can hinder network expressiveness. SELU's self-normalization properties prevent this issue and help maintain non-zero activations for most neurons throughout training.\n",
        "\n",
        "3) **Improved Generalization** : SELU has been shown to improve the generalization performance of deep neural networks. This means that models using SELU activations tend to generalize better to unseen data, potentially leading to better test accuracy."
      ],
      "metadata": {
        "id": "qnn3TMM7Qk03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4\n",
        "\n",
        "In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 4 -\n",
        "\n",
        "The choice of activation functions in neural networks depends on the specific characteristics of your problem, the architecture of your network, and the behavior you want to achieve. Here's a general guideline for when to use different activation functions:\n",
        "\n",
        "1) **SELU (Scaled Exponential Linear Unit)** :\n",
        "\n",
        "- Use SELU when you have a deep neural network (many layers) and you want to take advantage of its self-normalization properties.\n",
        "\n",
        "- Suitable for feedforward networks with many hidden layers.\n",
        "\n",
        "- Requires specific initialization (LeCun initialization) and architecture (no skip connections).\n",
        "\n",
        "2) **Leaky ReLU and its Variants (e.g., Parametric ReLU, Exponential Linear Unit - ELU)** :\n",
        "\n",
        "- Use Leaky ReLU or its variants when you want to address the \"dying ReLU\" problem and allow a small gradient for negative inputs.\n",
        "\n",
        "- Particularly useful when you have a deeper network and you want to mitigate the issue of neurons becoming inactive.\n",
        "\n",
        "- Leaky ReLU variants like Parametric ReLU and ELU offer different slopes and behaviors for negative inputs, providing flexibility in model learning.\n",
        "\n",
        "3) **ReLU (Rectified Linear Unit)** :\n",
        "\n",
        "- Use ReLU as a default choice for many hidden layers when training deep neural networks.\n",
        "\n",
        "- It's computationally efficient and helps in alleviating the vanishing gradient problem.\n",
        "\n",
        "- However, be cautious about the dying ReLU problem, especially when dealing with very deep networks.\n",
        "\n",
        "4) **Tanh (Hyperbolic Tangent)** :\n",
        "\n",
        "- Use tanh when you want to squash activations between -1 and 1.\n",
        "\n",
        "- It's often used in recurrent neural networks (RNNs) and convolutional neural networks (CNNs) for image processing.\n",
        "\n",
        "- It has zero-centered outputs, which can be useful in certain optimization scenarios.\n",
        "\n",
        "5) **Logistic (Sigmoid)** :\n",
        "\n",
        "- Use logistic (sigmoid) when you need binary classification, and you want to squash activations between 0 and 1.\n",
        "\n",
        "- It's commonly used in the output layer for binary classification problems.\n",
        "\n",
        "- Not suitable for deep networks due to vanishing gradient issues, except in the output layer for binary classification.\n",
        "\n",
        "6) **Softmax** :\n",
        "\n",
        "- Use softmax in the output layer when you're dealing with multi-class classification problems (more than two classes).\n",
        "\n",
        "- It converts raw scores (logits) into class probabilities, making it suitable for classification tasks."
      ],
      "metadata": {
        "id": "lMxcS5kFRDQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5\n",
        "\n",
        "What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 5 -\n",
        "\n",
        "Setting the momentum hyperparameter too close to 1, such as 0.99999, when using the Stochastic Gradient Descent (SGD) optimizer can lead to several issues, primarily related to training instability and convergence problems. Here are some of the potential consequences:\n",
        "\n",
        "1) **Slow Convergence** : When momentum is set very close to 1, it means that the moving average of past gradients has a strong influence on the current update direction. As a result, the updates become very slow, and the learning process may significantly decelerate. This can lead to long training times.\n",
        "\n",
        "2) **Overshooting and Oscillations** : Extremely high momentum values can cause the optimization process to overshoot the minimum of the loss function. The momentum term effectively accumulates a significant velocity in the previous directions of the gradient, and this momentum can carry the optimization process past the minimum, causing oscillations around the optimal point.\n",
        "\n",
        "3) **Convergence to Poor Local Minima** : The high momentum can lead the optimizer to \"skip\" over local minima and not settle in them, potentially missing better solutions. This can hinder the model's ability to find the best weights and biases for the given problem.\n",
        "\n",
        "4) **Difficulty in Escaping Local Minima** : Conversely, the high momentum can make it challenging for the optimizer to escape from sharp and narrow local minima. The momentum effectively \"locks\" the optimizer into a direction, and if that direction leads to a local minimum, the optimizer may struggle to explore other regions of the loss landscape.\n",
        "\n",
        "5) **Numerical Precision Issues** : Extremely high momentum values can lead to numerical precision issues, especially when using limited-precision representations (e.g., in deep learning frameworks with float16 or float32 data types). This can result in numerical instability during training."
      ],
      "metadata": {
        "id": "Xoio_BDLV-Pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6\n",
        "\n",
        "Name three ways you can produce a sparse model.\n",
        "\n",
        "..............\n",
        "\n",
        "Answer 6 -\n",
        "\n",
        "Producing a sparse model, which has fewer parameters than a dense (fully connected) model, can be beneficial for various reasons, including reducing memory and computation requirements and potentially improving model generalization. Here are three ways to produce a sparse model:\n",
        "\n",
        "1) **Weight Pruning** :\n",
        "\n",
        "- Weight pruning involves removing or setting certain model weights to zero based on specific criteria. It's a post-training technique.\n",
        "\n",
        "- Common approaches include magnitude-based pruning, where weights with magnitudes below a certain threshold are pruned, and structured pruning, where entire neurons or channels are pruned.\n",
        "\n",
        "- Pruning can be applied iteratively, gradually increasing the sparsity level while fine-tuning the remaining weights. - This process is called pruning and fine-tuning.\n",
        "Popular libraries like TensorFlow and PyTorch provide tools for weight pruning.\n",
        "\n",
        "2) **Sparse Activations (e.g., Sparse Autoencoders)** :\n",
        "\n",
        "- Instead of pruning weights, some models aim to produce sparse activations during training. Sparse autoencoders, for example, encourage a subset of neurons to activate while keeping others mostly inactive.\n",
        "\n",
        "- Techniques like L1 regularization on activation values or adding a sparsity loss term during training can encourage sparsity in activations.\n",
        "\n",
        "- Sparse activations can be useful for feature selection or dimensionality reduction.\n",
        "\n",
        "3) **Low-Rank Factorization** :\n",
        "\n",
        "- Low-rank factorization methods seek to factorize weight matrices into smaller matrices with reduced rank. This effectively reduces the number of parameters.\n",
        "\n",
        "- Singular Value Decomposition (SVD) is a common technique for low-rank factorization. It decomposes a weight matrix into three matrices (U, Σ, V), and then the middle diagonal matrix (Σ) is truncated to achieve a lower rank.\n",
        "\n",
        "- Tensor decomposition techniques like Tucker decomposition and CP decomposition can also be used for factorization."
      ],
      "metadata": {
        "id": "rG6VyMsoZB97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7\n",
        "\n",
        "Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
        "\n",
        "...............\n",
        "\n",
        "Answer 7 -\n",
        "\n",
        "Dropout is a regularization technique commonly used during training in neural networks to prevent overfitting. Dropout randomly sets a fraction of neurons' activations to zero during each training step. While dropout introduces some computational overhead during training, it can have different effects on training speed and inference speed:\n",
        "\n",
        "**`Training Speed with Dropout`** :\n",
        "\n",
        "- During training, dropout can slow down the training process. This is because, for each batch of data, dropout introduces randomness, and the forward and backward passes need to be computed with different subsets of neurons active.\n",
        "\n",
        "- The computational cost of dropout during training is generally manageable, especially with modern hardware and optimized deep learning frameworks. However, it does require additional computations compared to a model without dropout.\n",
        "\n",
        "**`Inference Speed with Dropout`** :\n",
        "\n",
        "- During inference (making predictions on new instances), dropout is typically turned off. Inference is usually faster without dropout because there is no random dropout of neurons, and the entire network is used for prediction.\n",
        "\n",
        "- The inference speed is usually a primary concern in production environments where you want to make predictions quickly.\n",
        "\n",
        "**`MC Dropout (Monte Carlo Dropout)`** :\n",
        "\n",
        "- MC Dropout is a technique used during inference to capture uncertainty estimates from a model trained with dropout.\n",
        "\n",
        "- Instead of turning off dropout entirely, MC Dropout involves running the model multiple times (e.g., 10 or more) with dropout enabled and averaging the predictions.\n",
        "\n",
        "- MC Dropout can slow down inference significantly compared to standard inference without dropout, as it involves running the model multiple times and averaging the results.\n",
        "\n",
        "However, MC Dropout provides valuable uncertainty estimates and can be useful in applications where understanding prediction uncertainty is important, such as in Bayesian deep learning or reinforcement learning."
      ],
      "metadata": {
        "id": "BnIuwIO2aLAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8\n",
        "\n",
        "Practice training a deep neural network on the CIFAR10 image dataset:\n",
        "\n",
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
        "\n",
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
        "\n",
        "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
        "\n",
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
        "\n",
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
        "\n",
        "...................\n",
        "\n",
        "Answer 10 -\n",
        "\n",
        "Below is the process for the same:\n",
        "\n",
        "a) **Build a DNN with 20 hidden layers of 100 neurons each using He initialization and the ELU activation function** :"
      ],
      "metadata": {
        "id": "OlUmhFnPcQ-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n",
        "\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DQptBjr3I_E",
        "outputId": "4b9cdfc3-2df6-4204-cf12-ebc285a0c608"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "Epoch 1/30\n",
            "1563/1563 [==============================] - 41s 16ms/step - loss: 2.0189 - accuracy: 0.2617 - val_loss: 1.8252 - val_accuracy: 0.3235\n",
            "Epoch 2/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.8261 - accuracy: 0.3310 - val_loss: 1.7913 - val_accuracy: 0.3348\n",
            "Epoch 3/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7681 - accuracy: 0.3561 - val_loss: 1.7551 - val_accuracy: 0.3594\n",
            "Epoch 4/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7107 - accuracy: 0.3815 - val_loss: 1.6735 - val_accuracy: 0.3947\n",
            "Epoch 5/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6699 - accuracy: 0.4022 - val_loss: 1.6668 - val_accuracy: 0.3958\n",
            "Epoch 6/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6389 - accuracy: 0.4124 - val_loss: 1.6700 - val_accuracy: 0.4022\n",
            "Epoch 7/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6144 - accuracy: 0.4238 - val_loss: 1.6225 - val_accuracy: 0.4206\n",
            "Epoch 8/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5907 - accuracy: 0.4336 - val_loss: 1.5866 - val_accuracy: 0.4422\n",
            "Epoch 9/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5685 - accuracy: 0.4429 - val_loss: 1.6031 - val_accuracy: 0.4216\n",
            "Epoch 10/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 2.1823 - accuracy: 0.3069 - val_loss: 1.7795 - val_accuracy: 0.3495\n",
            "Epoch 11/30\n",
            "1563/1563 [==============================] - 22s 14ms/step - loss: 1.7391 - accuracy: 0.3616 - val_loss: 1.6893 - val_accuracy: 0.3875\n",
            "Epoch 12/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7699 - accuracy: 0.3549 - val_loss: 1.7047 - val_accuracy: 0.3748\n",
            "Epoch 13/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6867 - accuracy: 0.3823 - val_loss: 1.7402 - val_accuracy: 0.3703\n",
            "Epoch 14/30\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.6534 - accuracy: 0.3956 - val_loss: 1.6978 - val_accuracy: 0.3908\n",
            "Epoch 15/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6330 - accuracy: 0.4091 - val_loss: 1.6038 - val_accuracy: 0.4180\n",
            "Epoch 16/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6174 - accuracy: 0.4168 - val_loss: 1.6426 - val_accuracy: 0.4073\n",
            "Epoch 17/30\n",
            "1563/1563 [==============================] - 23s 14ms/step - loss: 1.5950 - accuracy: 0.4238 - val_loss: 1.6082 - val_accuracy: 0.4166\n",
            "Epoch 18/30\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5746 - accuracy: 0.4322 - val_loss: 1.5835 - val_accuracy: 0.4262\n",
            "Epoch 19/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.9588 - accuracy: 0.3419 - val_loss: 1.7309 - val_accuracy: 0.3710\n",
            "Epoch 20/30\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.6912 - accuracy: 0.3824 - val_loss: 1.6634 - val_accuracy: 0.3985\n",
            "Epoch 21/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6381 - accuracy: 0.4061 - val_loss: 1.6476 - val_accuracy: 0.4099\n",
            "Epoch 22/30\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.6072 - accuracy: 0.4193 - val_loss: 1.6095 - val_accuracy: 0.4191\n",
            "Epoch 23/30\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 2.0080 - accuracy: 0.4007 - val_loss: 1.9666 - val_accuracy: 0.2448\n",
            "Epoch 24/30\n",
            "1563/1563 [==============================] - 23s 15ms/step - loss: 1.8419 - accuracy: 0.2940 - val_loss: 1.8940 - val_accuracy: 0.3026\n",
            "Epoch 25/30\n",
            "1563/1563 [==============================] - 25s 16ms/step - loss: 1.7840 - accuracy: 0.3329 - val_loss: 1.8150 - val_accuracy: 0.3310\n",
            "Epoch 26/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7218 - accuracy: 0.3573 - val_loss: 1.7190 - val_accuracy: 0.3688\n",
            "Epoch 27/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.6538 - accuracy: 0.3879 - val_loss: 1.6799 - val_accuracy: 0.3732\n",
            "Epoch 28/30\n",
            "1563/1563 [==============================] - 24s 16ms/step - loss: 1.6229 - accuracy: 0.4042 - val_loss: 1.6428 - val_accuracy: 0.4115\n",
            "Epoch 29/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5965 - accuracy: 0.4169 - val_loss: 1.7249 - val_accuracy: 0.3770\n",
            "Epoch 30/30\n",
            "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7569 - accuracy: 0.3937 - val_loss: 1.6892 - val_accuracy: 0.3863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) **Train the network using Nadam optimization and early stopping** :"
      ],
      "metadata": {
        "id": "PbK9EMnr5_OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convert pixel values to float and normalize\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the neural network architecture\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXGkP43S3J5h",
        "outputId": "3a0b589e-48e4-49a4-917b-19ece4701ed2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1407/1407 [==============================] - 67s 46ms/step - loss: 1.4559 - accuracy: 0.4766 - val_loss: 1.2431 - val_accuracy: 0.5644\n",
            "Epoch 2/50\n",
            "1407/1407 [==============================] - 61s 43ms/step - loss: 1.1213 - accuracy: 0.6086 - val_loss: 1.0937 - val_accuracy: 0.6234\n",
            "Epoch 3/50\n",
            "1407/1407 [==============================] - 60s 43ms/step - loss: 0.9987 - accuracy: 0.6509 - val_loss: 0.9725 - val_accuracy: 0.6652\n",
            "Epoch 4/50\n",
            "1407/1407 [==============================] - 62s 44ms/step - loss: 0.9158 - accuracy: 0.6805 - val_loss: 0.9340 - val_accuracy: 0.6806\n",
            "Epoch 5/50\n",
            "1407/1407 [==============================] - 60s 42ms/step - loss: 0.8461 - accuracy: 0.7052 - val_loss: 0.9059 - val_accuracy: 0.6910\n",
            "Epoch 6/50\n",
            "1407/1407 [==============================] - 60s 43ms/step - loss: 0.7865 - accuracy: 0.7260 - val_loss: 0.9483 - val_accuracy: 0.6774\n",
            "Epoch 7/50\n",
            "1407/1407 [==============================] - 58s 41ms/step - loss: 0.7360 - accuracy: 0.7448 - val_loss: 0.8938 - val_accuracy: 0.6978\n",
            "Epoch 8/50\n",
            "1407/1407 [==============================] - 58s 42ms/step - loss: 0.6877 - accuracy: 0.7596 - val_loss: 0.8694 - val_accuracy: 0.7110\n",
            "Epoch 9/50\n",
            "1407/1407 [==============================] - 58s 41ms/step - loss: 0.6439 - accuracy: 0.7763 - val_loss: 0.8752 - val_accuracy: 0.7048\n",
            "Epoch 10/50\n",
            "1407/1407 [==============================] - 58s 41ms/step - loss: 0.6058 - accuracy: 0.7893 - val_loss: 0.8642 - val_accuracy: 0.7194\n",
            "Epoch 11/50\n",
            "1407/1407 [==============================] - 58s 41ms/step - loss: 0.5666 - accuracy: 0.8019 - val_loss: 0.9298 - val_accuracy: 0.7030\n",
            "Epoch 12/50\n",
            "1407/1407 [==============================] - 58s 41ms/step - loss: 0.5351 - accuracy: 0.8116 - val_loss: 0.9279 - val_accuracy: 0.7070\n",
            "Epoch 13/50\n",
            "1407/1407 [==============================] - 57s 41ms/step - loss: 0.4997 - accuracy: 0.8229 - val_loss: 0.9639 - val_accuracy: 0.7056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Kuf8s9gJiQA",
        "outputId": "d2ccf7ee-72c0-4d75-a2bc-f63e95950735"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.7044000029563904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Add Batch Normalization and compare the learning curves:"
      ],
      "metadata": {
        "id": "a_1iVL6FvPeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convert pixel values to float and normalize\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the neural network architecture with Batch Normalization\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "# Set up early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history_bn = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5lEDQmT3KqB",
        "outputId": "de30f642-c6b1-4580-a0d5-5a78445e1928"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1407/1407 [==============================] - 88s 61ms/step - loss: 1.2796 - accuracy: 0.5491 - val_loss: 1.5355 - val_accuracy: 0.4748\n",
            "Epoch 2/50\n",
            "1407/1407 [==============================] - 81s 58ms/step - loss: 0.9684 - accuracy: 0.6630 - val_loss: 0.9738 - val_accuracy: 0.6624\n",
            "Epoch 3/50\n",
            "1407/1407 [==============================] - 79s 56ms/step - loss: 0.8332 - accuracy: 0.7099 - val_loss: 0.8714 - val_accuracy: 0.6984\n",
            "Epoch 4/50\n",
            "1407/1407 [==============================] - 80s 57ms/step - loss: 0.7343 - accuracy: 0.7434 - val_loss: 1.0480 - val_accuracy: 0.6424\n",
            "Epoch 5/50\n",
            "1407/1407 [==============================] - 80s 57ms/step - loss: 0.6494 - accuracy: 0.7728 - val_loss: 0.9802 - val_accuracy: 0.6664\n",
            "Epoch 6/50\n",
            "1407/1407 [==============================] - 80s 57ms/step - loss: 0.5737 - accuracy: 0.7997 - val_loss: 1.0340 - val_accuracy: 0.6698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss_bn, test_acc_bn = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy with Batch Normalization:\", test_acc_bn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwN2g1I1M99v",
        "outputId": "48ad4d64-6e65-41b3-b6b7-8e43460d9f4d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy with Batch Normalization: 0.6873999834060669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the learning curves\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Without Batch Normalization\")\n",
        "plt.plot(history_bn.history[\"val_accuracy\"], label=\"With Batch Normalization\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "HHmlX710NIC6",
        "outputId": "2ddd62dd-0e19-48fd-8e12-ab6325f68cad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Batch Normalization can help the model converge faster and produce a better model. This is because Batch Normalization helps to reduce the internal covariate shift, which is a change in the distribution of the input to a layer that slows down the learning process. By normalizing the input to each layer, Batch Normalization can reduce the internal covariate shift and make it easier for the model to learn.\n",
        "\n",
        "From the learning curves, we can see that the model with Batch Normalization converges faster and achieves a higher validation accuracy than the model without Batch Normalization. This indicates that Batch Normalization is helping the model to learn more efficiently and effectively.\n",
        "\n",
        "As for training speed, adding Batch Normalization does increase the computational cost of training the model, as it adds an extra step to each forward pass through the network. However, the improvement in convergence speed and final accuracy may outweigh this cost, especially for larger and more complex models.\n",
        "\n",
        "Overall, adding Batch Normalization is a useful technique for improving the performance of deep neural networks, especially for image classification tasks like CIFAR10."
      ],
      "metadata": {
        "id": "vRdMb5kJNQLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Replace Batch Normalization with SELU:"
      ],
      "metadata": {
        "id": "IXpTKPxCvdGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.layers import Activation\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize the input data\n",
        "X_train = normalize(X_train, axis=1)\n",
        "X_test = normalize(X_test, axis=1)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer\n",
        "optimizer = Nadam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnQjBE4hM8qU",
        "outputId": "59b03824-127f-4629-e8ab-bc7c29e75284"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rikG6NGi3LTs",
        "outputId": "f80378ee-e240-43af-a09a-8d7b9e70bb54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 283s 224ms/step - loss: 1.5074 - accuracy: 0.4833 - val_loss: 1.2676 - val_accuracy: 0.5649\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 282s 226ms/step - loss: 1.2525 - accuracy: 0.5678 - val_loss: 1.2078 - val_accuracy: 0.5784\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 275s 220ms/step - loss: 1.1996 - accuracy: 0.5861 - val_loss: 1.1421 - val_accuracy: 0.6075\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 283s 226ms/step - loss: 1.1648 - accuracy: 0.5949 - val_loss: 1.1433 - val_accuracy: 0.6104\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 278s 222ms/step - loss: 1.1301 - accuracy: 0.6071 - val_loss: 1.1100 - val_accuracy: 0.6132\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 268s 214ms/step - loss: 1.0918 - accuracy: 0.6220 - val_loss: 1.1630 - val_accuracy: 0.6214\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 269s 215ms/step - loss: 1.0627 - accuracy: 0.6310 - val_loss: 0.9880 - val_accuracy: 0.6639\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 273s 219ms/step - loss: 1.0243 - accuracy: 0.6466 - val_loss: 1.0795 - val_accuracy: 0.6459\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 276s 221ms/step - loss: 0.9914 - accuracy: 0.6610 - val_loss: 0.9649 - val_accuracy: 0.6770\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 286s 229ms/step - loss: 0.9585 - accuracy: 0.6709 - val_loss: 0.9365 - val_accuracy: 0.6821\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 282s 226ms/step - loss: 0.9321 - accuracy: 0.6833 - val_loss: 1.0660 - val_accuracy: 0.6685\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 272s 217ms/step - loss: 0.8968 - accuracy: 0.6957 - val_loss: 0.9967 - val_accuracy: 0.6961\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 278s 222ms/step - loss: 0.8732 - accuracy: 0.7060 - val_loss: 0.9601 - val_accuracy: 0.6984\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 273s 218ms/step - loss: 0.8546 - accuracy: 0.7159 - val_loss: 1.0283 - val_accuracy: 0.7061\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 270s 216ms/step - loss: 0.8355 - accuracy: 0.7222 - val_loss: 1.0005 - val_accuracy: 0.7024\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 272s 217ms/step - loss: 0.8032 - accuracy: 0.7367 - val_loss: 0.9638 - val_accuracy: 0.7023\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 276s 221ms/step - loss: 0.7868 - accuracy: 0.7426 - val_loss: 1.2980 - val_accuracy: 0.6764\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 274s 219ms/step - loss: 0.7671 - accuracy: 0.7509 - val_loss: 1.0826 - val_accuracy: 0.7018\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 268s 215ms/step - loss: 0.7508 - accuracy: 0.7584 - val_loss: 1.0578 - val_accuracy: 0.7107\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 268s 214ms/step - loss: 0.7391 - accuracy: 0.7641 - val_loss: 1.0405 - val_accuracy: 0.7008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy with SELU:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAgJ85hrnUgq",
        "outputId": "aedded14-586e-4a7a-e4a0-0daa0c974f7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 16s 51ms/step - loss: 0.9521 - accuracy: 0.6808\n",
            "Test accuracy with SELU: 0.6808000206947327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e) Try regularizing the model with alpha dropout and MC Dropout:"
      ],
      "metadata": {
        "id": "Jtj0X_BnvlRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, AlphaDropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.layers import Activation\n",
        "import numpy as np\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize the input data\n",
        "X_train = normalize(X_train, axis=1)\n",
        "X_test = normalize(X_test, axis=1)\n",
        "\n",
        "# Define the model architecture with alpha dropout\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    AlphaDropout(0.1),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    AlphaDropout(0.1),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    AlphaDropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with Nadam optimizer\n",
        "optimizer = Nadam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkyjlmCQkavS",
        "outputId": "5963ee5d-bcdf-4a87-e191-875e4cea61b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with alpha dropout and early stopping\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVkUP_0gvsLc",
        "outputId": "2f527deb-72f1-494c-a5e6-e2f2c5213130"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 282s 222ms/step - loss: 1.8604 - accuracy: 0.3380 - val_loss: 6.4020 - val_accuracy: 0.4162\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 278s 222ms/step - loss: 1.4941 - accuracy: 0.4727 - val_loss: 3.0859 - val_accuracy: 0.5650\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 275s 220ms/step - loss: 1.3112 - accuracy: 0.5415 - val_loss: 2.3505 - val_accuracy: 0.6232\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 272s 217ms/step - loss: 1.1981 - accuracy: 0.5857 - val_loss: 2.0300 - val_accuracy: 0.6461\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 274s 219ms/step - loss: 1.1241 - accuracy: 0.6113 - val_loss: 1.6731 - val_accuracy: 0.6750\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 266s 213ms/step - loss: 1.0452 - accuracy: 0.6409 - val_loss: 1.6622 - val_accuracy: 0.6740\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 264s 211ms/step - loss: 0.9909 - accuracy: 0.6582 - val_loss: 1.7162 - val_accuracy: 0.7030\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 264s 212ms/step - loss: 0.9348 - accuracy: 0.6811 - val_loss: 1.8058 - val_accuracy: 0.7108\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 269s 215ms/step - loss: 0.8890 - accuracy: 0.6951 - val_loss: 2.0377 - val_accuracy: 0.7139\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 263s 211ms/step - loss: 0.8421 - accuracy: 0.7140 - val_loss: 1.8335 - val_accuracy: 0.7172\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 270s 216ms/step - loss: 0.7967 - accuracy: 0.7308 - val_loss: 1.7683 - val_accuracy: 0.7015\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 299s 239ms/step - loss: 0.7825 - accuracy: 0.7373 - val_loss: 1.7228 - val_accuracy: 0.7249\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 304s 243ms/step - loss: 0.7365 - accuracy: 0.7535 - val_loss: 1.8570 - val_accuracy: 0.7049\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 302s 242ms/step - loss: 0.7116 - accuracy: 0.7619 - val_loss: 1.5031 - val_accuracy: 0.6988\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 305s 244ms/step - loss: 0.6919 - accuracy: 0.7692 - val_loss: 2.0864 - val_accuracy: 0.7221\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 299s 239ms/step - loss: 0.6663 - accuracy: 0.7798 - val_loss: 1.7665 - val_accuracy: 0.7191\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 303s 243ms/step - loss: 0.6507 - accuracy: 0.7870 - val_loss: 2.1621 - val_accuracy: 0.7293\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 298s 239ms/step - loss: 0.6237 - accuracy: 0.7950 - val_loss: 2.0893 - val_accuracy: 0.7240\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 300s 240ms/step - loss: 0.6153 - accuracy: 0.7992 - val_loss: 2.2154 - val_accuracy: 0.7042\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 296s 237ms/step - loss: 0.6024 - accuracy: 0.8053 - val_loss: 1.8515 - val_accuracy: 0.7005\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 296s 237ms/step - loss: 0.5935 - accuracy: 0.8089 - val_loss: 2.5669 - val_accuracy: 0.7208\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 296s 236ms/step - loss: 0.5744 - accuracy: 0.8149 - val_loss: 2.4218 - val_accuracy: 0.7234\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 303s 243ms/step - loss: 0.5632 - accuracy: 0.8184 - val_loss: 2.3287 - val_accuracy: 0.7252\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 302s 242ms/step - loss: 0.5638 - accuracy: 0.8197 - val_loss: 2.1786 - val_accuracy: 0.7253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy with alpha dropout:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpuYzuwf9yqq",
        "outputId": "2461b529-a57c-4552-d330-0c0e0c21a7e1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 18s 56ms/step - loss: 1.5133 - accuracy: 0.6932\n",
            "Test accuracy with alpha dropout: 0.6931999921798706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use MC Dropout for improved accuracy without retraining the model\n",
        "n_samples = 100\n",
        "y_probs = np.stack([model.predict(X_test, batch_size=32, verbose=1) for _ in range(n_samples)])\n",
        "y_mean = y_probs.mean(axis=0)\n",
        "y_std = y_probs.std(axis=0)\n",
        "y_pred = np.argmax(y_mean, axis=1)\n",
        "test_acc_mc = (y_pred == y_test.squeeze()).mean()\n",
        "print('Test accuracy with MC Dropout:', test_acc_mc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvXoj_U-91k4",
        "outputId": "233d3dab-2121-474e-d931-9f4d31ea3318"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 19s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 56ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 55ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 55ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 55ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 57ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 53ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 19s 61ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 59ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 56ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 17s 55ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "313/313 [==============================] - 18s 58ms/step\n",
            "313/313 [==============================] - 17s 54ms/step\n",
            "Test accuracy with MC Dropout: 0.6932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we can see that we achieved slightly better accuracy with MC Dropout (0.6932) compared to alpha dropout (0.6931) without retraining the model. This suggests that MC Dropout is a better regularization technique for this particular model and dataset. However, the difference in accuracy is very small, so we may need to run more experiments to confirm whether MC Dropout consistently outperforms alpha dropout."
      ],
      "metadata": {
        "id": "ZXXbIBdQTaw5"
      }
    }
  ]
}